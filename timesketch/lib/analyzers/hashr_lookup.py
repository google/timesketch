"""Sketch analyzer plugin for hashR lookup."""
from __future__ import unicode_literals

import logging
import random
import string
import sys

from flask import current_app
import sqlalchemy as sqla
from timesketch.lib.analyzers import interface, manager

logger = logging.getLogger('timesketch.analyzers.hashR')


class HashRLookupSketchPlugin(interface.BaseAnalyzer):
    """Sketch analyzer for HashRLookup."""

    NAME = 'hashR_lookup'
    DISPLAY_NAME = 'hashR lookup'
    DESCRIPTION = ('Lookup file-hash values in the known hashes database '
                   'generated by hashR.')

    DEPENDENCIES = frozenset()

    zerobyte_file_counter = 0
    known_hash_event_counter = 0
    unique_known_hash_counter = 0
    hashr_conn = None
    add_source_attribute = None

    def __init__(self, index_name, sketch_id, timeline_id=None):
        """Initialize The Sketch Analyzer.

        Args:
            index_name: Opensearch index name
            sketch_id: Sketch ID
            timeline_id: Timeline ID, default is None
        """
        self.index_name = index_name
        super().__init__(index_name, sketch_id, timeline_id=timeline_id)

    def connect_hashR(self):
        """Connect to the hashR postgres database.

        Returns:
          True: If connection is setup and succressfully tested.
          Error Message: If the connection cannot be setup or tested.

        Raises:
          Exception:  Raises an exception if the database connection details
                      cannot be loaded from the timesketch.conf file!
        """

        # TODO(setup): Provide connection infos in the data/timesketch.conf file!
        db_user = current_app.config.get('HASHR_DB_USER')
        db_pass = current_app.config.get('HASHR_DB_PW')
        db_address = current_app.config.get('HASHR_DB_ADDR')
        db_port = current_app.config.get('HASHR_DB_PORT')
        db_name = current_app.config.get('HASHR_DB_NAME')
        self.add_source_attribute = current_app.config.get(
            'HASHR_ADD_SOURCE_ATTRIBUTE')

        if (not db_user or not db_pass or not db_address or not db_port or
                not db_name or not self.add_source_attribute):
            msg = ('The hashR analyzer is not able to load the hashR database '
                   'information from the timesketch.conf file. Please make sure'
                   ' to uncomment the section and provide the required '
                   'connection details!')
            sys.tracebacklimit = 0
            raise Exception(msg)

        db_string = 'postgresql://{0:s}:{1:s}@{2:s}:{3:s}/{4:s}'.format(
            db_user, db_pass, db_address, db_port, db_name)
        db = sqla.create_engine(db_string, connect_args={
                                'connect_timeout': 10})
        try:
            db.connect()
            logger.info('Successful connected to hashR postgress database:'
                        ' postgresql://{0:s}:{1:s}@{2:s}:{3:s}/{4:s}'.format(
                            db_user, '***', db_address, db_port, db_name))
            self.hashr_conn = db
            return True
        except sqla.exc.OperationalError as err:
            logger.error(
                '!!! Connection to the hashR postgres database not '
                'possible! -- Provided connection string: "postgresql://'
                '{0:s}:{1:s}@{2:s}:{3:s}/{4:s} -- Error message: {5:s}'.format(
                    db_user, '***', db_address, db_port, db_name, str(err)))
            return ('Connection to the database FAILED. Please check the celery'
                    ' logs and make sure you have provided the correct database'
                    ' information in the analyzer file!')

    def check_against_hashR(self, sample_hashes):
        """Check a list of hashes against the hashR database.

        Args:
          sample_hashes:  A list or dict of hash values that shall be checked
                          against the hashR database.

        Returns:
          matching_hashes:  A list or dict containing all hashes that are found
                            in the hashR database. (If type<dict> it also
                            contains the corresponding source images.)
        """
        tmp_timeline_hashes_entries = []
        for sample_hash in sample_hashes:
            tmp_timeline_hashes_entries.append({'sha256': sample_hash})

        meta_data = sqla.MetaData(bind=self.hashr_conn)
        sqla.MetaData.reflect(meta_data)
        samples_table = meta_data.tables['samples']
        sources_table = meta_data.tables['sources']
        samples_sources_table = meta_data.tables['samples_sources']
        tmp_table_name = 'timeline_hashes_{}'.format(''.join(
            random.choice(string.ascii_lowercase) for i in range(8)))
        tmp_timeline_hashes_table = sqla.Table(
            tmp_table_name,
            meta_data,
            sqla.Column('sha256', sqla.String, primary_key=True),
            prefixes=['TEMPORARY'])

        matching_hashes = None

        with self.hashr_conn.connect() as conn:
            # Create new table and insert timeline hashes
            logger.info(
                'Create temporary table "{0:s}", inserting {1:,} unique hashes'
                .format(tmp_table_name, len(tmp_timeline_hashes_entries)))
            tmp_timeline_hashes_table.create(conn, checkfirst=True)
            insert_statement = tmp_timeline_hashes_table.insert(
                tmp_timeline_hashes_entries)
            conn.execute(insert_statement)

            if self.add_source_attribute:
                logger.info('ADD_SOURCE_ATTRIBUTE=True => going to add tags and'
                            ' attributes!')
                select_statement = sqla.select([
                    tmp_timeline_hashes_table.c.sha256,
                    samples_sources_table.c.source_sha256,
                    sources_table.c.sourceid, sources_table.c.reponame
                ]).select_from(
                    samples_sources_table.join(
                        tmp_timeline_hashes_table,
                        tmp_timeline_hashes_table.c.sha256 ==
                        samples_sources_table.c.sample_sha256).join(
                            sources_table, samples_sources_table.c.source_sha256
                            == sources_table.c.sha256))
                logger.info(
                    'Perform joined selection from {0:s}, samples_sources and '
                    'sources.'.format(tmp_table_name))
                results = conn.execute(select_statement)

                # Process all found hashes and tag the events accordingly
                hash_source_dict = {}
                logger.info('Extract sources from the results.')
                for entry in results:
                    sample_hash = entry[0]
                    if isinstance(entry[2], list):
                        source = '{}:{}'.format(entry[3], ';'.join(entry[2]))
                    else:
                        source = '{}:{}'.format(entry[3], entry[2])
                    hash_source_dict.setdefault(sample_hash, []).append(source)

                logger.info('Found {0:,} unique hashes in hashR DB.'.format(
                    len(hash_source_dict)))

                matching_hashes = hash_source_dict

            else:
                logger.info('ADD_SOURCE_ATTRIBUTE=False => adding tags only!')

                select_statement = sqla.select(
                    [samples_table.c.sha256]).select_from(
                        tmp_timeline_hashes_table.join(
                            samples_table, tmp_timeline_hashes_table.c.sha256 ==
                            samples_table.c.sha256))
                logger.info(
                    'Perform joined selection from {0:s} and samples.'.format(
                        tmp_table_name))
                results = conn.execute(select_statement)

                # Process all found hashes and tagg the events accordingly
                result_hash_list = []
                for entry in results:
                    self.unique_known_hash_counter += 1
                    result_hash_list.append(entry[0])

                logger.info('Found {0:,} unique hashes in hashR DB.'.format(
                    self.unique_known_hash_counter))
                matching_hashes = result_hash_list

        # close database engine
        self.hashr_conn.dispose()
        logger.info('Closed database conenction.')
        return matching_hashes

    def process_event(self, hash_value, sources, event):
        """Add tags and attributes to the given event, based on the rules for
        the analyzer.

        Args:
          hash_value: A string of a sha256 hash value.
          sources: A list of sources (image names) where this hash is known from.
          event:  The OpenSearch event object that contains this hash and needs
                  to be tagged or to add an attribute.

        Returns:
          None: If everything worked the function just returns.
        """
        tags_container = ['hashR']
        if (hash_value ==
                'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'
                ):
            tags_container.append('zerobyte file')
            self.zerobyte_file_counter += 1
            # Do not add any source attribute for zerobyte files,
            # since it exists in all sources.
            sources = False

        event.add_tags(tags_container)

        if sources:
            event.add_attributes({'hashR_sample_sources': sources})

        event.commit()

    def run(self):
        """Entry point for the analyzer.

        Returns:
            String with summary of the analyzer result
        """
        # Check if analyzer can connect to hashR:
        msg = self.connect_hashR()
        if isinstance(msg, str):
            return msg

        # TODO(setup): Add fieldnames that contain sha256 values in your events.
        query = 'hash_sha256:* OR sha256:* OR hash:* OR sha256_hash:*'

        # TODO(setup): Add fieldnames that contain sha256 values in your events.
        return_fields = ['hash_sha256', 'hash', 'sha256', 'sha256_hash']

        # Generator of events based on your query.
        # Swap for self.event_pandas to get pandas back instead of events.
        events = self.event_stream(
            query_string=query, return_fields=return_fields)
        known_hash_counter = 0
        error_hash_counter = 0
        total_event_counter = 0
        hash_events_dict = {}
        logger.info('Collect a list of unique hashes to check against hashR.')
        for event in events:
            total_event_counter += 1
            hash_value = None
            if 'hash_sha256' in event.source.keys():
                hash_value = event.source.get('hash_sha256')
            elif 'sha256' in event.source.keys():
                hash_value = event.source.get('sha256')
            elif 'hash' in event.source.keys():
                hash_value = event.source.get('hash')
            elif 'sha256_hash' in event.source.keys():
                hash_value = event.source.get('sha256_hash')
            else:
                error_hash_counter += 1
                logger.warning(
                    'No matching field with a hash found in this event! '
                    '-- Event Source: {0:s}'.format(str(event.source)))
                continue

            if len(hash_value) != 64:
                logger.warning('The extracted hash does not match the required '
                               'lenght (64) of a SHA256 hash. Skipping this '
                               'event! Hash: {0:s} - Lenght: {1:d}'.format(
                                   hash_value, len(hash_value)))
                error_hash_counter += 1
                continue

            hash_events_dict.setdefault(hash_value, []).append(event)

        if len(hash_events_dict) <= 0:
            return (
                'The selected timeline "{0:s}" does not contain any fields with'
                ' a sha256 hash.'
                .format(self.timeline_name))

        logger.info('Found {0:,} unique hashes in {1:,} events.'.format(
            len(hash_events_dict), total_event_counter))

        matching_hashes = self.check_against_hashR(hash_events_dict)
        if self.add_source_attribute:
            logger.info('Start adding tags and attributes to events.')
            for sample_hash in matching_hashes:
                for event in hash_events_dict[sample_hash]:
                    known_hash_counter += 1
                    self.process_event(
                        sample_hash, matching_hashes[sample_hash], event)
            self.unique_known_hash_counter = len(matching_hashes)
        else:
            logger.info('Start adding tags to events.')
            for sample_hash in matching_hashes:
                for event in hash_events_dict[sample_hash]:
                    known_hash_counter += 1
                    self.process_event(sample_hash, False, event)

        logger.info(
            'Analyzer summary: Found a total of {0:,} events with a '
            'sha256 hash value - {1:,} unique hashes queried against hashR - '
            '{2:,} hashes were known in hashR - {3:,} hashes were unknown '
            'in hashR - {4:,} events tagged - {5:,} entries were tagged '
            'as zerobyte files - {6:,} events raisend an error '.format(
                total_event_counter, len(hash_events_dict),
                self.unique_known_hash_counter,
                (len(hash_events_dict) - self.unique_known_hash_counter),
                known_hash_counter, self.zerobyte_file_counter,
                error_hash_counter))

        return (
            'Found a total of {0:,} events with a sha256 hash value - {1:,} '
            'unique hashes queried against hashR - {2:,} hashes were known in '
            'hashR - {3:,} hashes were unknown in hashR - {4:,} events tagged '
            '- {5:,} entries were tagged as zerobyte files - {6:,} events '
            'raisend an error'
            .format(total_event_counter, len(hash_events_dict),
                    self.unique_known_hash_counter,
                    (len(hash_events_dict) - self.unique_known_hash_counter),
                    known_hash_counter, self.zerobyte_file_counter,
                    error_hash_counter))


manager.AnalysisManager.register_analyzer(HashRLookupSketchPlugin)
